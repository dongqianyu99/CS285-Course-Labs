### 1. 问题的目标与直观理解

**目标**:
我们要证明的不等式是：
$$
\sum_{s_t} |p_{\pi_\theta}(s_t) - p_{\pi^*}(s_t)| \le 2T\epsilon
$$

**直观理解**:
*   $p_{\pi_\theta}(s_t)$：在时间步 $t$，由我们训练的**模仿策略 $\pi_\theta$** 所引导产生的状态分布。
*   $p_{\pi^*}(s_t)$：在时间步 $t$，由**专家策略 $\pi^*$** 所引导产生的状态分布。
*   $|p_{\pi_\theta}(s_t) - p_{\pi^*}(s_t)|$：这是两个概率分布在状态 $s_t$ 上的差值的绝对值。
*   $\sum_{s_t} |\dots|$：这个求和是对所有可能的状态 $s_t$ 进行的。这个表达式是两个概率分布之间**总变差距离 (Total Variation Distance)** 的两倍。总变差距离是衡量两个概率分布差异的一种标准方式。
*   **核心含义**: 这个不等式表明，模仿策略的状态分布与专家策略的状态分布之间的**差异（即分布偏移 Distributional Shift）**，其上界（worst-case bound）会随着**任务长度 $T$** 和**单步误差 $\epsilon$** 的乘积**线性增长**。

这个结论是非常可怕的。即使你的单步模仿误差 $\epsilon$ 非常小（比如1%），如果任务很长（比如 $T=100$），那么状态分布的差异可能会非常大 ($2 \times 100 \times 0.01 = 2$)。总变差距离的最大值就是2，这意味着两个分布可能已经**完全不重叠**了。

---

### 2. 证明思路详解 (Following the Hints)

我们将按照题目给出的提示，一步步地构建证明。

#### **基础设定**

我们从上一问知道，行为克隆（BC）的假设是：
$$
\mathbb{E}_{p_{\pi^*}(s)} [\pi_\theta(a \neq \pi^*(s) | s)] \le \epsilon
$$
这意味着在**专家访问过的状态**上，我们的策略犯错的概率平均不超过 $\epsilon$。

**Hint 1** 提示我们，在课堂上证明过一个基于更强假设的版本，即对于**任意**专家状态 $s_t \in \text{supp}(p_{\pi^*})$, 都有 $\pi_\theta(a \neq \pi^*(s) | s) \le \epsilon$。而我们现在面对的是一个更弱的、基于**期望**的假设。我们需要将这个期望形式的假设应用到证明中。

#### **步骤1: 定义误差事件**

让我们来定义一个关键的事件。我们关心的是模仿策略的轨迹何时开始偏离专家的轨迹。

令 $\tau_\theta = (s_0, a_0, \dots, s_T)$ 为模仿策略 $\pi_\theta$ 生成的一条轨迹。
令 $\tau^* = (s_0^*, a_0^*, \dots, s_T^*)$ 为专家策略 $\pi^*$ 生成的一条轨迹。
我们假设它们从相同的初始状态 $s_0 = s_0^*$ 开始。

两条轨迹在时间步 $t$ **发生偏离**，意味着在 $t$ 之前它们都完全一致，但在 $t$ 这一步，模仿策略做出了与专家不同的动作。

形式化地，我们定义一个事件 $E_t$：
$$
E_t: \quad a_i = a_i^* \text{ for all } i < t, \text{ but } a_t \neq a_t^*
$$
这个事件表示“在前 $t-1$ 步都模仿成功，但在第 $t$ 步首次犯错”。

#### **步骤2: 关联状态分布差异与误差事件**

现在，我们考虑在时间步 $t+1$ 的状态分布差异 $|p_{\pi_\theta}(s_{t+1}) - p_{\pi^*}(s_{t+1})|$。

一个重要的洞察是：如果直到时间步 $t$ 之前，模仿策略 $\pi_\theta$ 的轨迹**从未偏离**过专家策略 $\pi^*$ 的轨迹（即事件 $E_0, \dots, E_t$ 都没有发生），那么在时间步 $t+1$，两个策略所处的状态分布**必然是相同的**，即 $p_{\pi_\theta}(s_{t+1}) = p_{\pi^*}(s_{t+1})$。

因此，两个状态分布的差异**完全是由至少发生了一次偏离**所导致的。

也就是说，在时间步 $t+1$ 访问到某个状态 $s_{t+1}$ 的概率差异，可以被“在 $t+1$ 之前至少发生过一次错误”这个事件的概率所约束。

令 $H_t = \bigcup_{i=0}^{t} E_i$ 表示“在前 $t$ 步中至少犯错一次”的事件。
那么，两个分布的总变差距离可以被这个事件的概率上界所约束：
$$
\frac{1}{2} \sum_{s_{t+1}} |p_{\pi_\theta}(s_{t+1}) - p_{\pi^*}(s_{t+1})| \le \Pr[H_t]
$$

#### **步骤3: 使用 Union Bound (Hint 2)**

现在我们来计算 $\Pr[H_t]$ 的上界。根据 **Union Bound 不等式**：
$$
\Pr[H_t] = \Pr[\bigcup_{i=0}^{t} E_i] \le \sum_{i=0}^{t} \Pr[E_i]
$$

#### **步骤4: 计算单步误差事件的概率 $\Pr[E_i]$**

现在我们来计算在第 $i$ 步**首次**犯错的概率 $\Pr[E_i]$。
事件 $E_i$ 发生，意味着在前 $i-1$ 步都没有犯错，并且在第 $i$ 步犯错了。
我们可以将其写成条件概率：
$$
\Pr[E_i] = \Pr(a_i \neq a_i^* | a_0 = a_0^*, \dots, a_{i-1} = a_{i-1}^*)
$$
由于在前 $i-1$ 步都没有犯错，所以在第 $i$ 步的状态 $s_i$ 的分布与专家在该步的状态分布 $p_{\pi^*}(s_i)$ 是**相同的**。
因此，我们可以将上述概率写成在专家状态分布下的期望：
$$
\Pr[E_i] = \mathbb{E}_{s_i \sim p_{\pi^*}(s_i)} [\pi_\theta(a_i \neq \pi^*(s_i) | s_i)]
$$
这个期望正是在我们初始的**BC假设**中提到的单步误差！

#### **步骤5: 整合所有步骤**

我们将所有步骤串联起来：

1.  在时间步 $t$ 的状态分布差异（总变差距离）的上界是“在前 $t-1$ 步中至少犯错一次”的概率：
    $$
    \frac{1}{2} \sum_{s_t} |p_{\pi_\theta}(s_t) - p_{\pi^*}(s_t)| \le \Pr[\bigcup_{i=0}^{t-1} E_i]
    $$

2.  使用 Union Bound：
    $$
    \Pr[\bigcup_{i=0}^{t-1} E_i] \le \sum_{i=0}^{t-1} \Pr[E_i]
    $$

3.  根据步骤4，我们知道每一步首次犯错的概率 $\Pr[E_i]$ 是在该步专家状态分布下的期望误差。而根据我们的BC假设，所有时间步的**平均**期望误差小于等于 $\epsilon$。我们可以用这个平均值来近似（作为上界）每一步的误差：
    $$
    \Pr[E_i] = \mathbb{E}_{p_{\pi^*}(s_i)} [\pi_\theta(a_i \neq \pi^*(s_i) | s_i)] \le T \cdot \frac{1}{T} \sum_{k=0}^{T-1} \mathbb{E}_{p_{\pi^*}(s_k)} [\dots] = T\epsilon
    $$
所以，在时间步 $t$ 的状态分布差异：
$$
\frac{1}{2} \sum_{s_t} |p_{\pi_\theta}(s_t) - p_{\pi^*}(s_t)| \le \sum_{i=0}^{t-1} \Pr[E_i] \le \sum_{i=0}^{T-1} \Pr[E_i] \le T\epsilon
$$
将 2 乘过去，我们得到：
$$
\sum_{s_t} |p_{\pi_\theta}(s_t) - p_{\pi^*}(s_t)| \le 2T\epsilon
$$
这就完成了证明。

---

### **结论**

我们通过定义“首次犯错”事件，并利用Union Bound不等式，成功地将模仿策略与专家策略的状态分布差异（总变差距离）与单步模仿误差 $\epsilon$ 和任务长度 $T$ 关联起来。证明的核心在于认识到，分布的差异完全是由策略犯错导致的，而每次犯错的概率都可以被我们的初始模仿学习假设所约束。最终，我们得到了一个线性增长的上界 $2T\epsilon$，这从数学上严格地证明了行为克隆算法中复合误差的严重性。