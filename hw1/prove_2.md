### **问题设定**

*   **期望回报 $J(\pi)$**: 我们用 $J(\pi)$ 来衡量一个策略 $\pi$ 的好坏。它是在该策略下，整个任务（长度为 T）所能获得的**总期望奖励**。
    $$
    J(\pi) = \sum_{t=1}^{T} \mathbb{E}_{p_\pi(s_t)} [r(s_t)]
    $$
    这个公式计算的是，在策略 $\pi$ 的引导下，对每一个时间步 $t$ 可能出现的状态 $s_t$ 的奖励求期望，然后将所有时间步的期望奖励加起来。
*   **奖励函数 $r(s_t)$**: 奖励只和当前状态有关，并且是有界的，即 $|r(s_t)| \le R_{\max}$。
*   **性能损失**: 我们关心的是模仿策略与专家策略的期望回报之差，$J(\pi^*) - J(\pi_\theta)$。这个值越大，说明我们的模仿策略性能越差。

### **(a) 任务 (a)：奖励只在最后一步**

这种情况非常特殊，它模拟的是**目标导向任务（Goal-oriented tasks）**，比如“把钥匙插进锁孔”。你只有在最后一步成功到达目标状态时，才会得到奖励，中间过程没有奖励。

**目标**: 证明 $J(\pi^*) - J(\pi_\theta) = \mathcal{O}(T\epsilon)$。

**证明思路**:

1.  **写出性能损失的表达式**:
    根据 $J(\pi)$ 的定义，以及本问的特殊条件（$r(s_t)=0$ for $t < T$），性能损失为：
    $$
    J(\pi^*) - J(\pi_\theta) = \mathbb{E}_{p_{\pi^*}(s_T)} [r(s_T)] - \mathbb{E}_{p_{\pi_\theta}(s_T)} [r(s_T)]
    $$
    由于奖励只在最后一步 $T$ 产生，我们只关心在第 $T$ 步的状态分布。

2.  **将期望表达式展开**:
    期望可以写成在所有状态上对“概率 $\times$ 奖励”求和：
    $$
    J(\pi^*) - J(\pi_\theta) = \sum_{s_T} p_{\pi^*}(s_T) r(s_T) - \sum_{s_T} p_{\pi_\theta}(s_T) r(s_T)
    $$
    $$
    = \sum_{s_T} (p_{\pi^*}(s_T) - p_{\pi_\theta}(s_T)) r(s_T)
    $$

3.  **使用绝对值进行放缩**:
    我们对上式取绝对值，并利用不等式 $|\sum x_i y_i| \le \sum |x_i| |y_i|$：
    $$
    |J(\pi^*) - J(\pi_\theta)| = \left| \sum_{s_T} (p_{\pi^*}(s_T) - p_{\pi_\theta}(s_T)) r(s_T) \right|
    $$
    $$
    \le \sum_{s_T} |p_{\pi^*}(s_T) - p_{\pi_\theta}(s_T)| |r(s_T)|
    $$

4.  **引入已知条件**:
    *   我们知道奖励是有界的：$|r(s_T)| \le R_{\max}$。
    *   我们在上一问刚刚证明了状态分布差异的上界：$\sum_{s_t} |p_{\pi_\theta}(s_t) - p_{\pi^*}(s_t)| \le 2t\epsilon$。对于最后一步 $t=T$，我们有：
        $$
        \sum_{s_T} |p_{\pi^*}(s_T) - p_{\pi_\theta}(s_T)| \le 2T\epsilon
        $$

5.  **代入并得出结论**:
    将这两个条件代入第3步的不等式：
    $$
    |J(\pi^*) - J(\pi_\theta)| \le R_{\max} \sum_{s_T} |p_{\pi^*}(s_T) - p_{\pi_\theta}(s_T)|
    $$
    $$
    \le R_{\max} \cdot (2T\epsilon)
    $$
    由于 $R_{\max}$ 是一个常数，我们可以用大 $\mathcal{O}$ 符号来表示这个关系：
    $$
    J(\pi^*) - J(\pi_\theta) = \mathcal{O}(T\epsilon)
    $$
    **证明完成。**

**直观解释**: 在这种目标导向任务中，性能损失与任务长度 $T$ **线性相关**。因为错误只会影响最后一步的奖励，所以之前步骤中犯的错，只是增加了最后一步“跑偏”的概率。这个概率的累积是线性的。

---

### **(b) 任务 (b)：任意奖励函数**

这种情况更普遍，智能体在任务的**每一步**都可能获得奖励。

**目标**: 证明 $J(\pi^*) - J(\pi_\theta) = \mathcal{O}(T^2\epsilon)$。

**证明思路**:

1.  **写出性能损失的表达式**:
    现在，我们需要考虑所有时间步的奖励：
    $$
    J(\pi^*) - J(\pi_\theta) = \sum_{t=1}^{T} \left( \mathbb{E}_{p_{\pi^*}(s_t)} [r(s_t)] - \mathbb{E}_{p_{\pi_\theta}(s_t)} [r(s_t)] \right)
    $$

2.  **对每一项进行与(a)中类似的分析**:
    我们先看求和符号里面的部分，即在**单个时间步 $t$** 的性能损失。完全重复 (a) 中的步骤2到5，我们可以得到：
    $$
    |\mathbb{E}_{p_{\pi^*}(s_t)} [r(s_t)] - \mathbb{E}_{p_{\pi_\theta}(s_t)} [r(s_t)]| \le R_{\max} \sum_{s_t} |p_{\pi^*}(s_t) - p_{\pi_\theta}(s_t)|
    $$
    利用上一问的结论，我们知道在时间步 $t$ 的状态分布差异上界是 $2t\epsilon$：
    $$
    \le R_{\max} \cdot (2t\epsilon)
    $$

3.  **对所有时间步求和**:
    现在，我们将这个**单步性能损失的上界**代回到第1步的总性能损失表达式中：
    $$
    |J(\pi^*) - J(\pi_\theta)| = \left| \sum_{t=1}^{T} (\dots) \right| \le \sum_{t=1}^{T} |(\dots)|
    $$
    $$
    \le \sum_{t=1}^{T} R_{\max} (2t\epsilon)
    $$
    $$
    = 2 R_{\max} \epsilon \sum_{t=1}^{T} t
    $$

4.  **计算等差数列的和**:
    我们知道 $\sum_{t=1}^{T} t = \frac{T(T+1)}{2}$。
    $$
    |J(\pi^*) - J(\pi_\theta)| \le 2 R_{\max} \epsilon \cdot \frac{T(T+1)}{2} = R_{\max} \epsilon T(T+1)
    $$
    $$
    = R_{\max} (\epsilon T^2 + \epsilon T)
    $$

5.  **用大 $\mathcal{O}$ 符号表示**:
    当 $T$ 很大时，$T^2$ 是主导项。因此，我们可以写成：
    $$
    J(\pi^*) - J(\pi_\theta) = \mathcal{O}(T^2\epsilon)
    $$
    **证明完成。**

**直观解释**: 当每一步都有奖励时，性能损失与任务长度 $T$ 的**平方**相关。这是复合误差最可怕的体现。为什么是平方？
*   **第一个 $T$**: 来自于**误差的累积**。随着时间推移，模仿策略的状态分布会越来越偏离专家，这个偏离程度与 $t$ 线性相关。
*   **第二个 $T$**: 来自于**损失的累加**。因为每一步的偏离都会导致这一步的奖励损失，我们需要把这 $T$ 步的损失全部加起来。
*   所以，一个**随时间线性增长的单步损失**，在**整个时间跨度上累加**起来，最终就导致了与 $T^2$ 相关的总损失。

### **结论**

这个证明是行为克隆（Behavior Cloning）理论分析的终点，它清晰地展示了：
*   **复合误差的灾难性后果**: 一个微不足道的单步模仿误差 $\epsilon$，在长时程任务中会被放大到与 $T^2$ 相关的巨大性能差距。
*   **行为克隆的适用范围**: BC 算法只适用于任务长度 $T$ 相对较短，或者能够容忍较大性能损失的场景。
*   **后续算法的动机**: 正是因为这个 $T^2$ 的误差界，才促使研究者们开发了 DAgger、GAIL 等更先进的模仿学习算法，它们通过与环境的交互来修正分布偏移问题，从而将误差界降低到 $\mathcal{O}(T\epsilon)$。